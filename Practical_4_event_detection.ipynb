{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Zp828amoqAG"
   },
   "source": [
    "# Practical 3: Data analysis / Event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hr2FTxGTiBQ-"
   },
   "outputs": [],
   "source": [
    "# Environment setup code\n",
    "\n",
    "# Loads some modules for later use\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install Yapsy\n",
    "!pip install pomegranate==0.12.0\n",
    "!pip install tabel\n",
    "\n",
    "# Installs the Percption Engineer's toolbox to the working directory\n",
    "!git clone https://fahrensiesicher@bitbucket.org/fahrensiesicher/perceptionengineerstoolkit.git\n",
    "%cd perceptionengineerstoolkit\n",
    "\n",
    "# Run this line to download the data (in file data.txt) to your Colab runtime environment.\n",
    "!wget https://github.com/kueblert/gazeinteraction/raw/master/practical2-data.zip\n",
    "!unzip practical2-data.zip -d datasets\n",
    "!rm practical2-data.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn9RFhjb70IJ"
   },
   "source": [
    "#Question 1: Data quality\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO7NYNsI_0Hz"
   },
   "source": [
    "This exercise sheet will guide you through the typical workflow of eye-tracking data analysis. \n",
    "\n",
    "You have been given eye movement data from expert and novice dentists viewing teeth x-rays. Your job is to properly preprocess the data for analysis of the fixation and saccade behavior. \n",
    "\n",
    "We will use the *Perception Engineer's Toolbox* for this purpose. It contains a set of basic functions specific for the analysis of eye movement data (Documentation: https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/). These functions build off the knowledge you gained in the Vorlesung on event detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqXx85ABcwhJ"
   },
   "outputs": [],
   "source": [
    "# Have a look at the data\n",
    "!head -n 5 \"datasets/wct_013b - subject02.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSwC6D7h77RX"
   },
   "source": [
    "**Question 1.a. (1 point):** The toolbox works by chaining up commands. You can queue and execute commands as in the code sample below. Please adjust it so that it successfully loads the provided example data. You can find a description of the possible parameter for the CSV import plugin [here](https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/PerceptionToolkit/plugins/PersistenceCSV.py).\n",
    "\n",
    "Make the toolbox load all provided data files one after another (in the correct order 01-10).\n",
    "Carefully tell the toolbox how invalid data is marked so that it is recognized correctly as being invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mtHU7XavpOh"
   },
   "outputs": [],
   "source": [
    "# Code snippet for reading and parseing raw data file. refer to the documentation for better descriptions of the \n",
    "# plugins used\n",
    "from PerceptionToolkit.PEPluginManager import PEPluginManager\n",
    "from PerceptionToolkit.CommandProcessor import *\n",
    "\n",
    "plugin_manager = PEPluginManager() \n",
    "controller = CommandProcessor()\n",
    "\n",
    "PersistenceCSV_plugin = CommandProcessor.find_plugin(plugin_manager, \"PersistenceCSV\")\n",
    "\n",
    "#TODO: add the corresponding header values for the toolkit to parse the appropriate data\n",
    "my_aliases = {\"TIME\": \"...\",\n",
    "            \"LEFT_EYE_X\": \"...\",\n",
    "            \"LEFT_EYE_Y\": \"...\",\n",
    "            \"RIGHT_EYE_X\": \"...\",\n",
    "            \"RIGHT_EYE_Y\": \"...\",\n",
    "            \"LEFT_PUPIL_DIAMETER\": \"...\",\n",
    "            \"RIGHT_PUPIL_DIAMETER\": \"...\"}\n",
    "            \n",
    "#for each file, call the command to run the read function from the persistenceCSV_plugin\n",
    "for filename in sorted(os.listdir(\"datasets\")):\n",
    "  print(filename)\n",
    "  cmd = Command(PersistenceCSV_plugin, \"read\", {\n",
    "        # TODO adjust the import definitions to work with the data  \n",
    "        \"aliases\": my_aliases,\n",
    "        \"filename\": \"datasets/%s\"%filename,\n",
    "        }) \n",
    "  \n",
    "  controller.execute_command(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAJ8bRBqHz4D"
   },
   "source": [
    "**Question 1.b. (2 points):** We have already learned to carefully check data **quality** and **consistency**. \n",
    "\n",
    "To start off, visualize **all** the tracked samples using the [VisualizationDataQuality](https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/PerceptionToolkit/plugins/VisualizationDataQuality.py) plugin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTW56kcPmpXH"
   },
   "outputs": [],
   "source": [
    "# No need to change contents of this snippet, just run and interpret the output.\n",
    "# Needs data to be loaded into the toolbox via previous code snippet.\n",
    "VisualizationDataQuality_plugin = CommandProcessor.find_plugin(plugin_manager, \"VisualizationDataQuality\")\n",
    "cmd = Command(VisualizationDataQuality_plugin, \"\", {\"img_height\": 500})\n",
    "controller.execute_command(cmd)\n",
    "\n",
    "# Display code (no need to change)\n",
    "from IPython.display import Image\n",
    "Image(\"/content/perceptionengineerstoolkit/img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18PDLum_jLpU"
   },
   "source": [
    "Now remove any inconsistent data from the provided trials using the TrialFilterQuality and TrialFilterProperty plugins. As with the other plugins, you can access their sources/documentation [here](https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/PerceptionToolkit/plugins/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h93q6etd9s85"
   },
   "outputs": [],
   "source": [
    "# Check the quality of each trial using the \"Trial Filter Quality\" and the \"Trial Filter Property\" commands.\n",
    "# set a minimum tracking ratio to 95% (0.95) for the filter quality\n",
    "\n",
    "cmd = #TODO: Quality\n",
    "controller.execute_command(cmd)\n",
    "#will return the number of trials removed due to low quality\n",
    "\n",
    "cmd = #TODO: Property\n",
    "controller.execute_command(cmd)\n",
    "#will return the number of trial properties not matching\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBxohZo9954j"
   },
   "source": [
    "**Question 1.b (3 points):** Now visualize *only the successfully tracked samples* using the [VisualizationDataQuality](https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/PerceptionToolkit/plugins/VisualizationDataQuality.py) plugin.\n",
    "\n",
    "*Hint:* No need to change the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F182m1I_1dlp"
   },
   "outputs": [],
   "source": [
    "#No need to change contents of this snippet, just run and interpret the output\n",
    "VisualizationDataQuality_plugin = CommandProcessor.find_plugin(plugin_manager, \"VisualizationDataQuality\")\n",
    "cmd = Command(VisualizationDataQuality_plugin, \"\", {\"img_height\": 500})\n",
    "controller.execute_command(cmd)\n",
    "\n",
    "# Display code (no need to change)\n",
    "from IPython.display import Image\n",
    "Image(\"/content/perceptionengineerstoolkit/img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_1M_hOn1eqb"
   },
   "source": [
    "In one to two sentences, describe the nature of the differences in data quality between the samples.\n",
    "> **Answer:** your answer here\n",
    "\n",
    "In one sentence, hypothesize about their possible causes during data recording.\n",
    "> **Answer:** your answer here\n",
    "\n",
    "List which data you would consider for exclusion for the further analysis, give a short explaination for each?\n",
    "> **Answer:** your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXbuykDaBb3e"
   },
   "source": [
    "# Question 2: Event detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7lLvg7MJ4PP"
   },
   "source": [
    "**Question 2.a. (5 points):** One of the most reliable and commonly used methods for event detection is the I-VT. It thresholds the eye movement velocity and marks time slices with large movement velocity as saccades, others as fixations. \n",
    "\n",
    "In theory, there is only a single parameter, the velocity threshold.\n",
    "\n",
    "But...\n",
    "\n",
    "In practice, there are *plenty*. Use the [I-VT](https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/PerceptionToolkit/plugins/EventdetectionIVT.py) to identify fixations and saccades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZRp9iTD5XRw"
   },
   "outputs": [],
   "source": [
    "#TODO: use the EventDetectionIVT_plugin, and determine the appropriate parameters, to run the I-VT\n",
    "# This will require: \n",
    "# -> Eye movement velocity information (from vorlesung)\n",
    "# -> positional information:\n",
    "# We can assume the subject was sitting **600mm** away from the eyetracker, which was attached to a 15.6\" (34.42cm x 19.35cm (w x h)) screen size with 1920 x 1080 pixels.\n",
    "# -> the toolbox \"documentation\"\n",
    "\n",
    "# will return number of fixations and saccades for both eyes combined for the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugOyg1JO5WlJ"
   },
   "source": [
    "In one to two sentences, how do you choose the velocity threshold?\n",
    "> **Answer:** Your answer here.\n",
    "\n",
    "Briefly explain what is one advantage of calculating eye velocity over a time window as done by the provided implementation?\n",
    "> **Answer:** Your answer here.\n",
    "\n",
    "Why is there an option to filter short fixations (*hint*: What is the intended interpretation of a fixation)? Support your response in one sentence.\n",
    "> **Answer:** Your answer here.\n",
    "\n",
    "Why would one want to merge subsequent/close-by fixations? Is this more likely useful for large or for small velocity thresholds? Breifly support your response.\n",
    "> **Answer:** Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad4cKOgititO"
   },
   "source": [
    "**Question 2.b. (1 point)** Even good data quality is not perfect. For the following algorithms (such as eye movement event detection) to work smoothly, we *still* need to preprocess the data. First, we remove any gaps (= sequences of invalid samples) in the data that are only of very short duration (= a few consecutive samples). These samples will be interpolated linearly from the neighboring samples. Utilize the PreprocessGapFill plugin for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCSCoe_09-Mp"
   },
   "outputs": [],
   "source": [
    "#TODO: apply preprocessGapFill_plugin\n",
    "#will return gaps filled for both eyes for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwX3z50A3YVi"
   },
   "source": [
    "**Question 2.c. (2 points):**\n",
    "Not every false measurement is marked as such. In reality, it extremely difficult for the eye-tracker to decipher a correct pupil from a similar-looking black spot e.g. clumpy mascara. \n",
    "\n",
    "To control for signal errors, you can **smooth the data slightly using a moving median filter**. Use the PreprocessMedianFilter plugin for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TI5rdfz34ZT"
   },
   "outputs": [],
   "source": [
    "#TODO: apply preprocessMedianFilter_plugin\n",
    "#will return applied median filter to the events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIBX7t7I34mZ"
   },
   "source": [
    "There is also a `PreprocessMovingAverageFilter` plugin. In one sentence, describe what they do differently. In another sentence, shortly discuss the advantages and disadvantages of both and how they handle outliers.\n",
    "> **Answer:** Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOuW-xTzv4fd"
   },
   "source": [
    "**Question 2.d. (1 point):** Now the data has gone through some basic pre-processing steps. We can rerun the I-VT to see the cleaned-up events. These events are then ready to be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Shljtssh9a1A"
   },
   "outputs": [],
   "source": [
    "#TODO: rerun EventDetectionIVT_plugin\n",
    "#profit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aks26uaCqurG"
   },
   "source": [
    "Compare the results to the above ones without prior filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5Mdnz0VHl8H"
   },
   "source": [
    "---\n",
    "# Question 3: Visualization (Bonus, do this one last, after Question 4!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku-mb0TO0i-A"
   },
   "source": [
    "Visualizing the data can give you a good indication of the attention distribution, which can help you further develop research questions.\n",
    "\n",
    "The bee swarm, or *scanpath visualization*, displays a circle at the location of each fixation. Fixations are interconnected using lines/arrows that show the transition from one fixation to another.\n",
    "\n",
    "The size of the fixation circle is determined by the fixation duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UuXbV8EH1w6"
   },
   "source": [
    "**Question 3.a. (6 points):** Implement a plugin that plots the loaded data as a bee swarm visualization. Then show the 3rd and 4th (last expert and first student), of the remaining 8 trials, as bee swarms. Be sure to use two different colors!\n",
    "\n",
    "*Tip:* Do not include 0's (invalid data) in the visualization and gaze samples that are outside the image dimensions (i.e. negative values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58ESaiTV2JEM"
   },
   "outputs": [],
   "source": [
    "# This will write the plugin file to the correct location.\n",
    "%%writefile PerceptionToolkit/plugins/VisualizationBeeSwarm.py\n",
    "from PerceptionToolkit.PluginInterfaces import IVisualizationPlugin\n",
    "from PerceptionToolkit.EyeMovements import Fixation, Saccade\n",
    "from PerceptionToolkit.DataModel import DataModel\n",
    "from PerceptionToolkit.Version import Version\n",
    "import cv2 #<- for drawing the circles and lines\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure, show\n",
    "from typing import Dict, Any, Sequence\n",
    "\n",
    "\n",
    "class VisualizationBeeSwarm(IVisualizationPlugin):\n",
    "    def __init__(self):\n",
    "        #DO NOT CHANGE\n",
    "        super(VisualizationBeeSwarm, self).__init__()\n",
    "        self.img_height: int = int(600)\n",
    "        self.img_width: int = int(800)\n",
    "        self.draw_trial: list = [3,4]\n",
    "    \n",
    "    def apply_parameters(self, parameters: Dict[str, Any]) -> None:\n",
    "        #DO NOT CHANGE \n",
    "        self.img_height = parameters.get(\"img_height\", self.img_height)\n",
    "        self.img_width = parameters.get(\"img_width\", self.img_width)\n",
    "        self.draw_trial = parameters.get(\"draw_trial\", self.draw_trial)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def version() -> Version:\n",
    "        #DO NOT CHANGE\n",
    "        return Version(0, 1, 0)\n",
    "\n",
    "    def draw(self, data: Sequence[DataModel]) -> np.array:\n",
    "        #TODO: input a data set, and fill an np.array (the aggregated image) \n",
    "        #use appropriate opencv drawing functions to represent fixations and saccades \n",
    "        \n",
    "\n",
    "        return np.array(aggregated_img).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hur_fAnC6J9u"
   },
   "source": [
    "Run the following snippet as is. It makes your plugin known to the toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfczyHaJcco-"
   },
   "outputs": [],
   "source": [
    "%%writefile PerceptionToolkit/plugins/VisualizationBeeSwarm.toolbox-plugin\n",
    "[Core]\n",
    "Name = VisualizationBeeSwarm\n",
    "Module = VisualizationBeeSwarm\n",
    "\n",
    "[Documentation]\n",
    "Description = Visualizes a scanpath\n",
    "Author = student\n",
    "Version = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ql38yCCxLfLC"
   },
   "outputs": [],
   "source": [
    "# It is probably necessary to reload the plugin manager to get to know the new plugin.\n",
    "plugin_manager = PEPluginManager() \n",
    "#TODO: define visualizationBeeSwarm_plugin, to create the bee swarms for display\n",
    "# and execute it here.\n",
    "\n",
    "\n",
    "# Display code (no need to change)\n",
    "from IPython.display import Image\n",
    "Image(\"/content/perceptionengineerstoolkit/img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpITVlKJq-AP"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drRe_Up2HvwM"
   },
   "source": [
    "# Question 4: Metrics and their interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VGJui-k9Q2w"
   },
   "source": [
    "As you know from the lecture, eye movement events provide insight into cognitive and other attentional differences. *These differences can be evident between groups, such as experts and novices, as is the case with our data*.\n",
    "\n",
    "Generally, analysis of eye movement events and their differences between groups is done on a larger scale (i.e. more than 10 participants) and then average behavior of these events is compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCcmE9JdKBpi"
   },
   "source": [
    "**Question 4.a. (3 points):** The [MetricFixation](https://bitbucket.org/fahrensiesicher/perceptionengineerstoolkit/src/dev/PerceptionToolkit/plugins/MetricFixation.py) plugin will provide several accumulated eye movement statistics. Calculate these statistics for the experts (subjects 1-5) and for the students (subjects 6-10)\n",
    "\n",
    "*Tip:* remember to remove the datasets from the analysis that were deemed bad quality in Question 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD2C11Hd-UcK"
   },
   "outputs": [],
   "source": [
    "# TODO:Load expert data\n",
    "# Preprocess as above (gap, median, IVT)\n",
    "# apply MetricFixation_plugin, which returns expert statistics\n",
    "\n",
    "# TODO:Load student data\n",
    "# Preprocess as above (gap, median, IVT)\n",
    "# apply MetricFixation_plugin, which returns expert statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3eijsAPH3TB"
   },
   "source": [
    "**Question 4.b. (6 points):** \n",
    "\n",
    "**So, what do the differences mean?** That's one of the major tasks to find out when analyzing eye-tracking data. There are common interpretations for each metric, however they may change from domain to domain and depend on the experimental setting.\n",
    "\n",
    "**Do a brief literature review on eye movements in expertise studies**. Which metrics are meaningful in that context? How can they be interpreted? And are they consistent with your findings?\n",
    "\n",
    "Fill in the example table below with your sources, and include the citations in the provided area underneath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFwwbk0Zp4h5"
   },
   "source": [
    "> Answer:\n",
    "\n",
    "Reference | metric examined and finding | consistency with your findings\n",
    "--- | --- | ---\n",
    "[1] | Average fixation duration oscillates during expertise development | Contradicts our findings, but the seriousness of the source can be questioned. Fixation duration is associated with cognitive processing of the looked-at object.\n",
    "[2] | foo | lorem ipsum\n",
    "[3] | bar | dolor sit amet\n",
    "...\n",
    "\n",
    "[1] Eye movement development during learning how to eat gummi bears with chopsticks, Tom & Cherry, 1980, Journal of Cartoons:\n",
    "\n",
    "> Further detailled notes could go here, if you need the additional space to clarify.\n",
    "\n",
    "[2]\n",
    "\n",
    "[3]\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_3_event_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
